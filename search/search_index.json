{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview The purpose of this site is to facilitate researchers in the use of the Snowmass21 Connect service in support of the Snowmass2021 effort . As a user you have access to the following services: A gateway to the Open Science Grid . You can submit jobs to the OSG in order to run and scale computational workflows across the nationally distributed High ThroughPut Computing resource. Access to storage. You can stage input files and collect output from your jobs. An environment for the development of OSG appropriate workflows. Access to installed software and the means to make them available on the grid. A local computing platform for non-grid interactive jobs to either test code or analyse data. Access the Connect node This section provides information on: How to obtain an account How to login to the resource How to manage that account Job Submissions This section provides an introduction on how to submit jobs to the OSG from the Snowmass21 Connect node. Addittional software-specific examples are included in the software section. Data Management on Connect node This section provides information on: Storage locations Transfering files Data on the grid Access to Software This section provides information on how to access software on the Snowmass21 Connect node. It includes information on the OSG module environment and a guidance on running installed software in support of the the Snowmass21 effort - organized between local jobs on the node and jobs on the grid where available. Support and Consultation The Snowmass21 Connect service is supported by the University of Chicago and the Open Science Grid. To report issues with the service or to request a consultation on submitting and running jobs on the OpenScienceG Grid please submit a ticket to help@opensciencegrid.org . Additional support is available in the #snowmass-connect channel at http://snowmass2021.slack.com.","title":"Overview"},{"location":"#overview","text":"The purpose of this site is to facilitate researchers in the use of the Snowmass21 Connect service in support of the Snowmass2021 effort . As a user you have access to the following services: A gateway to the Open Science Grid . You can submit jobs to the OSG in order to run and scale computational workflows across the nationally distributed High ThroughPut Computing resource. Access to storage. You can stage input files and collect output from your jobs. An environment for the development of OSG appropriate workflows. Access to installed software and the means to make them available on the grid. A local computing platform for non-grid interactive jobs to either test code or analyse data.","title":"Overview"},{"location":"#access-the-connect-node","text":"This section provides information on: How to obtain an account How to login to the resource How to manage that account","title":"Access the Connect node"},{"location":"#job-submissions","text":"This section provides an introduction on how to submit jobs to the OSG from the Snowmass21 Connect node. Addittional software-specific examples are included in the software section.","title":"Job Submissions"},{"location":"#data-management-on-connect-node","text":"This section provides information on: Storage locations Transfering files Data on the grid","title":"Data Management on Connect node"},{"location":"#access-to-software","text":"This section provides information on how to access software on the Snowmass21 Connect node. It includes information on the OSG module environment and a guidance on running installed software in support of the the Snowmass21 effort - organized between local jobs on the node and jobs on the grid where available.","title":"Access to Software"},{"location":"#support-and-consultation","text":"The Snowmass21 Connect service is supported by the University of Chicago and the Open Science Grid. To report issues with the service or to request a consultation on submitting and running jobs on the OpenScienceG Grid please submit a ticket to help@opensciencegrid.org . Additional support is available in the #snowmass-connect channel at http://snowmass2021.slack.com.","title":"Support and Consultation"},{"location":"accounting/","text":"Access the Connect node Obtain an account In order to access the Snowmass21 Connect node you will need to sign up for an account. To sign up, visit https://connect.snowmass21.io . You will need to authenticate via CILogon to your institutional sign in page before you proceed. One your membership request is approved you will be notified by email. Important steps: You need to upload your ssh-keys following the instructions on the portal. The process will create your home directory on the Snowmass submit node and grant you access via passwordless ssh. You must use an insitutional email when you sign up. Applications using personal email addresses will be rejected. In order to submit jobs to the OSG you should also follow up - once your account request is approved - with a request to be added to a particular subgroup: snowmass21.energy or snowmass21.cosmic. An email back will confirm your addition to your requested subgroup project. To do that navigate to the groups section in the portal here: https://snowmass21.ci-connect.net/groups and select a particular subgroup to join. Login to the Submit Node Once you upload your ssh-keys in the Snowmass21 Connect portal and your account request has been processed and approved, it takes a few minutes for your home directory to be created. You can then connect to the login node via passwordless ssh as: ssh <user_id>@login.snowmass21.io You can find your <user_id> from your profile on the Snowmass21 Connect portal. The login node is also a submission node for jobs to the Open Science Grid. Upon login you will land in your home directory /home/<user_id> . Your home directory has 50GB of quota. Use your home directory to store job submission files, scripts and code. Do not store large files (larger than 300 MB) in your home directory for the purpose of submissions to the OSG. If you need an interactive environment to support export of graphics back to your machine, you would need to use the -X or -Y flag in your ssh command: ssh -X <user_id>@login.snowmass21.io You would also need to make your terminal X11 compatible. For machines running MacOS a useful tool can be found here Manage your account You can come back to the portal if you need to change your institutional affiliation, email, join another subgroup or upload additional ssh-keys to your account. Account management is controlled by an automated process which will only make ssh-keys persistent in your user account if they are uploaded through the portal.","title":"Access the Connect node"},{"location":"accounting/#access-the-connect-node","text":"","title":"Access the Connect node"},{"location":"accounting/#obtain-an-account","text":"In order to access the Snowmass21 Connect node you will need to sign up for an account. To sign up, visit https://connect.snowmass21.io . You will need to authenticate via CILogon to your institutional sign in page before you proceed. One your membership request is approved you will be notified by email. Important steps: You need to upload your ssh-keys following the instructions on the portal. The process will create your home directory on the Snowmass submit node and grant you access via passwordless ssh. You must use an insitutional email when you sign up. Applications using personal email addresses will be rejected. In order to submit jobs to the OSG you should also follow up - once your account request is approved - with a request to be added to a particular subgroup: snowmass21.energy or snowmass21.cosmic. An email back will confirm your addition to your requested subgroup project. To do that navigate to the groups section in the portal here: https://snowmass21.ci-connect.net/groups and select a particular subgroup to join.","title":"Obtain an account"},{"location":"accounting/#login-to-the-submit-node","text":"Once you upload your ssh-keys in the Snowmass21 Connect portal and your account request has been processed and approved, it takes a few minutes for your home directory to be created. You can then connect to the login node via passwordless ssh as: ssh <user_id>@login.snowmass21.io You can find your <user_id> from your profile on the Snowmass21 Connect portal. The login node is also a submission node for jobs to the Open Science Grid. Upon login you will land in your home directory /home/<user_id> . Your home directory has 50GB of quota. Use your home directory to store job submission files, scripts and code. Do not store large files (larger than 300 MB) in your home directory for the purpose of submissions to the OSG. If you need an interactive environment to support export of graphics back to your machine, you would need to use the -X or -Y flag in your ssh command: ssh -X <user_id>@login.snowmass21.io You would also need to make your terminal X11 compatible. For machines running MacOS a useful tool can be found here","title":"Login to the Submit Node"},{"location":"accounting/#manage-your-account","text":"You can come back to the portal if you need to change your institutional affiliation, email, join another subgroup or upload additional ssh-keys to your account. Account management is controlled by an automated process which will only make ssh-keys persistent in your user account if they are uploaded through the portal.","title":"Manage your account"},{"location":"data_management_main/","text":"Data Management This section provides a list of important information on how you can manage your data on the Snowmass21 login node. Storage locations on Connect node Storage locations available to the users on the Snowmass21 connect node: Home directory. Your home directory, /home/<user_id> , has 50GB of storage available. It is recommended to use it for storing scripts, submission files and small size data. Large input files for jobs on the grid should not be stored here. Local private storage in /work/<user_id> . Each user directory has a quota of 5 TB to temporarily store data and build your own submission pipeline to the OSG. It can also be used as your private work area for local analysis or processing jobs on the login node. Local shared storage in /project/data . This directory is for you to place data that needs to be shared with other users. Users can write there any data needed by multiple users and different jobs on the grid. Stash storage is accebible from the login node at /collab and is intented for datasets larger than 1GB each for/from jobs or for distribution to external institutions over http or globus. There two subdirectories in /collab : For private user data in: /collab/user/<user_id> . Each user directory has a 1 TB quota. For shared data the Snowmass21 project members in: /collab/project/snowmass21/data , a 50TB allocation to serve the project storage needs. Transferring data You can transfer data from external institutions to the Snowmass21 Connect using any of the three following methods: scp . For example: scp -r <file_or_directory> <user_id>@login.snowmass21.io:/work/<user_id>/. will copy a file or a directory from your local machine to your user directory in local storage. The ssh-keys used for your profile on the Snowmass Connect portal must stored on the local machine. rsync . For example: rsync -avz -e \"ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\" --progress test.transfer <user_id>@login.snowmass21.io:dump/ will copy the test.transfer file in the /home/<user_id>/dump/ directory. If the directory does not exist, it will be created. As in scp the ssh-keys used for your profile on the Snowmass Connect portal must stored on the source machine. Globus Connect can be used to transfer files to/from stash storage only. Instructions on how to set up Globus Connect Personal can be found here . Access to the stash storage endpoint is enabled by authenticating against the Globus collection \"OSG Connect CI Logon\" using the GLobus Connect client. You can search for the collection by name in the search bar of the File Manager. In order to access the stash storage on the Snowmass login node via Globus online, you must have an institutional based grid certificate issued by CILogon. To obtain one follow the steps below: Logon with your institutional credentials at http://cilogon.org Select \"Create a Password Protected Certificate\". Enter a password and download your encrypted certificate, named usercred.p12. The certificate can be obtained by using the openssl pcks12 command as: openssl pkcs12 -in [your-cert-file] -clcerts -nokeys -out usercert.pem Email paschos@uchicago.edu the output of the following command which will print out your DN (Distinguish Name): openssl x509 -in usercert.pem -noout -subject Once your DN has been entered in the user access list you will be able to access the OSG Connect CI Logon collection with the Globus Connect client by validating with your institution credentials. Navigate to the OSG Snowmass21 Collaborations Connect storage by typing in the Path box /cephfs/osg/collab . You can then navigate to your user directory as shown in the example below: Shown in the image above are two possible destinations for the data. Navigate to /cephfs/osg/collab/project/snowmass21 if data are to be shared by multiple users. Navigate to /cephfs/osg/user/<user_id> if data are for the exclusive use of a single user. In both cases, users can create subdirectories and organize content by either using the Globus client interface or from the login.snowmass21.io node. On the right panel of the Globus Connect client tool you can search and connect to another collection. The latter can be your own laptop/server or a collaboration end point that has provided a Globus Connect door for the researchers to use. To transfer files you can select the list files from your local computer and then select Start. To transfer files out simply reverse the direction of the process. Important : You can not access home and work directories on the login server over the Globus door. Since you have access to the /stash/collab directory, you can login to login.snowmass21.io and move or copy files over to your home or work directory. Data for grid jobs There are four methods for the user to make data available to remote sites running their jobs. HTCondor File Transfer. This method is recommended for the majority of computational workflows running on the OSG. Users can employ this method if the total size of the input data per job does not exceed 1 GB. In addition, OSG recommends that the output data per job that need to be transfered back does not exceed 1 GB as well. To enable HTCondor File transfers for your input and output data insert the following parameters anywhere in your HTCondor submit file: transfer_input_files = <comma separated files or directories> transfer_output_files = <comma separated files or directories> This method can leverage any storage location on the Snowmass21 Connect node. However it is recommended that you primarily use /work/<user_id> and avoid the /home/<user_id> . OSG's StashCache. To use this service, data should be placed either in /collab/user/<user_id> or /collab/project/snowmass21 . This method is recommended for input files larger than 1 GB each or 10 GB total from all input data. The recommended upper limit for the output files to be transfered back from the remote node is 10 GB per job. Users can use the stashcp tool to transfer data from their /collab space only to the remote host. You can insert the following command in your execution script to transfer data from /collab/user/<user_id> to the local directory on the remote worker node where your job is running: module load stashcache stashcp /osgconnect/collab/user/<user_id>/<input_file> . To transfer data back to your collab space from the remote node run the following command in your execution script: stashcp <output_file> stash:///osgconnect/collab/user/<user_id>/<output_file> If the filesize of each input dataset exceeds 10 GB then an alternative method for transfers is the GridFTP protocol using the gfal-copy tool. Please reach out for a consultation to discuss if your workflow can benefit from access to a GridFTP door. Transfers over HTTP. Files stored in the shared namespace, /collab/project/snowmass21 are public and also accessible via HTTP. To access datta there you can use linux tools like wget as shown in the following example: wget http://stash.osgconnect.net/collab/project/snowmass21/<file_name> You can insert such command in your execution script to download datasets on the remote worker node where your job is running. Alternatively, you can declare those files inside your HTCondor submission script as follows: transfer_input_files = http://stash.osgconnect.net/collab/project/snowmass21/<file_name> HTTP based transfers are best for filesizes up to 1GB.","title":"Data Management"},{"location":"data_management_main/#data-management","text":"This section provides a list of important information on how you can manage your data on the Snowmass21 login node.","title":"Data Management"},{"location":"data_management_main/#storage-locations-on-connect-node","text":"Storage locations available to the users on the Snowmass21 connect node: Home directory. Your home directory, /home/<user_id> , has 50GB of storage available. It is recommended to use it for storing scripts, submission files and small size data. Large input files for jobs on the grid should not be stored here. Local private storage in /work/<user_id> . Each user directory has a quota of 5 TB to temporarily store data and build your own submission pipeline to the OSG. It can also be used as your private work area for local analysis or processing jobs on the login node. Local shared storage in /project/data . This directory is for you to place data that needs to be shared with other users. Users can write there any data needed by multiple users and different jobs on the grid. Stash storage is accebible from the login node at /collab and is intented for datasets larger than 1GB each for/from jobs or for distribution to external institutions over http or globus. There two subdirectories in /collab : For private user data in: /collab/user/<user_id> . Each user directory has a 1 TB quota. For shared data the Snowmass21 project members in: /collab/project/snowmass21/data , a 50TB allocation to serve the project storage needs.","title":"Storage locations on Connect node"},{"location":"data_management_main/#transferring-data","text":"You can transfer data from external institutions to the Snowmass21 Connect using any of the three following methods: scp . For example: scp -r <file_or_directory> <user_id>@login.snowmass21.io:/work/<user_id>/. will copy a file or a directory from your local machine to your user directory in local storage. The ssh-keys used for your profile on the Snowmass Connect portal must stored on the local machine. rsync . For example: rsync -avz -e \"ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\" --progress test.transfer <user_id>@login.snowmass21.io:dump/ will copy the test.transfer file in the /home/<user_id>/dump/ directory. If the directory does not exist, it will be created. As in scp the ssh-keys used for your profile on the Snowmass Connect portal must stored on the source machine. Globus Connect can be used to transfer files to/from stash storage only. Instructions on how to set up Globus Connect Personal can be found here . Access to the stash storage endpoint is enabled by authenticating against the Globus collection \"OSG Connect CI Logon\" using the GLobus Connect client. You can search for the collection by name in the search bar of the File Manager. In order to access the stash storage on the Snowmass login node via Globus online, you must have an institutional based grid certificate issued by CILogon. To obtain one follow the steps below: Logon with your institutional credentials at http://cilogon.org Select \"Create a Password Protected Certificate\". Enter a password and download your encrypted certificate, named usercred.p12. The certificate can be obtained by using the openssl pcks12 command as: openssl pkcs12 -in [your-cert-file] -clcerts -nokeys -out usercert.pem Email paschos@uchicago.edu the output of the following command which will print out your DN (Distinguish Name): openssl x509 -in usercert.pem -noout -subject Once your DN has been entered in the user access list you will be able to access the OSG Connect CI Logon collection with the Globus Connect client by validating with your institution credentials. Navigate to the OSG Snowmass21 Collaborations Connect storage by typing in the Path box /cephfs/osg/collab . You can then navigate to your user directory as shown in the example below: Shown in the image above are two possible destinations for the data. Navigate to /cephfs/osg/collab/project/snowmass21 if data are to be shared by multiple users. Navigate to /cephfs/osg/user/<user_id> if data are for the exclusive use of a single user. In both cases, users can create subdirectories and organize content by either using the Globus client interface or from the login.snowmass21.io node. On the right panel of the Globus Connect client tool you can search and connect to another collection. The latter can be your own laptop/server or a collaboration end point that has provided a Globus Connect door for the researchers to use. To transfer files you can select the list files from your local computer and then select Start. To transfer files out simply reverse the direction of the process. Important : You can not access home and work directories on the login server over the Globus door. Since you have access to the /stash/collab directory, you can login to login.snowmass21.io and move or copy files over to your home or work directory.","title":"Transferring data"},{"location":"data_management_main/#data-for-grid-jobs","text":"There are four methods for the user to make data available to remote sites running their jobs. HTCondor File Transfer. This method is recommended for the majority of computational workflows running on the OSG. Users can employ this method if the total size of the input data per job does not exceed 1 GB. In addition, OSG recommends that the output data per job that need to be transfered back does not exceed 1 GB as well. To enable HTCondor File transfers for your input and output data insert the following parameters anywhere in your HTCondor submit file: transfer_input_files = <comma separated files or directories> transfer_output_files = <comma separated files or directories> This method can leverage any storage location on the Snowmass21 Connect node. However it is recommended that you primarily use /work/<user_id> and avoid the /home/<user_id> . OSG's StashCache. To use this service, data should be placed either in /collab/user/<user_id> or /collab/project/snowmass21 . This method is recommended for input files larger than 1 GB each or 10 GB total from all input data. The recommended upper limit for the output files to be transfered back from the remote node is 10 GB per job. Users can use the stashcp tool to transfer data from their /collab space only to the remote host. You can insert the following command in your execution script to transfer data from /collab/user/<user_id> to the local directory on the remote worker node where your job is running: module load stashcache stashcp /osgconnect/collab/user/<user_id>/<input_file> . To transfer data back to your collab space from the remote node run the following command in your execution script: stashcp <output_file> stash:///osgconnect/collab/user/<user_id>/<output_file> If the filesize of each input dataset exceeds 10 GB then an alternative method for transfers is the GridFTP protocol using the gfal-copy tool. Please reach out for a consultation to discuss if your workflow can benefit from access to a GridFTP door. Transfers over HTTP. Files stored in the shared namespace, /collab/project/snowmass21 are public and also accessible via HTTP. To access datta there you can use linux tools like wget as shown in the following example: wget http://stash.osgconnect.net/collab/project/snowmass21/<file_name> You can insert such command in your execution script to download datasets on the remote worker node where your job is running. Alternatively, you can declare those files inside your HTCondor submission script as follows: transfer_input_files = http://stash.osgconnect.net/collab/project/snowmass21/<file_name> HTTP based transfers are best for filesizes up to 1GB.","title":"Data for grid jobs"},{"location":"job_submission/","text":"Job Submissions to the OSG This section provides a short introduction on how to submit jobs to the OSG from the Snowmass Connect login node using the snowmass21.energy subgroup as the project name. A minimal HTCondor submission script, myjob.submit , to the OSG is inlined below: Universe = Vanilla Executable = run.sh Error = output.err.$(Cluster)-$(Process) Output = output.out.$(Cluster)-$(Process) Log = output.log.$(Cluster) should_transfer_files = YES WhenToTransferOutput = ON_EXIT request_cpus = 1 request_memory = 1 GB +ProjectName=\"snowmass21.energy\" Queue 1 Refer to the HTCondor manual for more information on the declared parameters and on customizing your submission scripts. When the HTCondor script above is submitted, you request a remote worker node with 1 core and 1 GB to run the run.sh executable. In this case, run.sh is a shell script that contains a list of commands that executes your workload on the worker node. For example: #/bin/bash ./code_executable <input_file> <output_file> <additional commands> The parameter should_transfer_files = YES instructs HTCondor to use the HTCondor file transfer method to transfer the Executable to the remote host and the job files Error (stderr) , Output (stdout) and Log back to your directory on the submit host. You will have a number of options to transfer code executables and input/output files to the remote worker node, described in the next section. You can submit the job script to the OSG via the HTCondor command on the Snowmass login node as: condor_submit myjob.submit , which will return a unique <JobID> number. You can use the <JobID> to query the status of your job with condor_q <JobID> For an introduction on managing your jobs with HTCondor we refer to this presentation by the OSG. Job Submission Guidelines If your application/code was built or depends on modules used on the snowmass21 login node and it dynamically links against libraries of the module environment you would need to ensure that these modules are also availablle and loaded on the remote worker node. To do so: Insert the following parameter in your submission script: Requirements = (HAS_MODULES =?= TRUE) . This will request a worker node on a site where the OSG modules are available Before you invoke your executable inside the run.sh script load the modules as: module load module1 module2 We discuss modules with a little bit more detail in here . You must always populate the project name field, e.g. +ProjectName=\"snowmass21.energy\" , in your HTCondor submission script to: Ensure your job is validated for HTCondor to run it on the OSG grid Job statistics are properly collected and displayed on the OSG monitoring dashboard for the snowmass project. The Tutorial Command OSG provides a list of tutorials available as repositories on Github. These tutorials are designed for OSG submit nodes, are tested regularly and should work as is, but if you experience any issues please contact us. Users on the Snowmass login node should be able to run most of those. List of options for the tutorial command From the Snowmass21 Connect login node, the following tutorial commands are available: $ tutorial $ tutorial list $ tutorial info <tutorial-name> $ tutorial <tutorial-name> Available Tutorials The following tutorials pre-installed. Some additional tutorials specific to Snowmass21 will be deployed there as well. Tutorials that will not work on the Snowmass login node at present are struckthrough. To see what is currently available: $ tutorial list Currently available tutorials: R ...................... Estimate Pi using the R programming language R-addlibSNA ............ Shows how to add R external libraries for the R jobs on the OSG ScalingUp-Python ....... Scaling up compute resources - Python example to optimize a function on grid points annex .................. None blast-split ............ How to run BLAST on the OSG by splitting a large input file connect-client ......... Demonstrates how to use the connect client for remote job submission dagman-wordfreq ........ DAGMan based wordfreq example dagman-wordfreq-stash .. DAGMan based wordfreq - data from stash error101 ............... Use condor_q -better-analyze to analyze stuck jobs exitcode ............... Use HTCondor's periodic_release to retry failed jobs htcondor-transfer ...... Transfer data via HTCondor's own mechanisms matlab-HelloWorld ...... Creating standalone MATLAB application - Hello World nelle-nemo ............. Running Nelle Nemo's goostats on the grid oasis-parrot ........... Software access with OASIS and Parrot octave ................. Matrix manipulation via the Octave programming language osg-locations .......... Tutorial based on OSG location exercise from the User School pegasus ................ An introduction to the Pegasus job workflow manager photodemo .............. A complete analysis workflow using HTTP transfer quickstart ............. How to run your first OSG job root ................... Inspect ntuples using the ROOT analysis framework scaling ................ Learn to steer jobs to particular resources scaling-up-resources ... A simple multi-job demonstration software ............... Software access tutorial tensorflow-matmul ...... Tensorflow math operations as a singularity container job on the OSG - matrix multiplication Tutorials stash-cvmfs and stash-http do not work on the Snowmass Connect Login node at present. We are working to replace with working variants. Install and Setup a Tutorial On the Snowmass Connect login node, create a directory, cd to it, and invoke the command: $ tutorial <tutorial-name> This command will clone the tutorial repository to your current working directory. cd to the repository directory and follow the steps described in the readme.md file. Alternatively, you can view the readme.md file at the tutorial's corresponding GitHub page.","title":"Job Submissions to the OSG"},{"location":"job_submission/#job-submissions-to-the-osg","text":"This section provides a short introduction on how to submit jobs to the OSG from the Snowmass Connect login node using the snowmass21.energy subgroup as the project name. A minimal HTCondor submission script, myjob.submit , to the OSG is inlined below: Universe = Vanilla Executable = run.sh Error = output.err.$(Cluster)-$(Process) Output = output.out.$(Cluster)-$(Process) Log = output.log.$(Cluster) should_transfer_files = YES WhenToTransferOutput = ON_EXIT request_cpus = 1 request_memory = 1 GB +ProjectName=\"snowmass21.energy\" Queue 1 Refer to the HTCondor manual for more information on the declared parameters and on customizing your submission scripts. When the HTCondor script above is submitted, you request a remote worker node with 1 core and 1 GB to run the run.sh executable. In this case, run.sh is a shell script that contains a list of commands that executes your workload on the worker node. For example: #/bin/bash ./code_executable <input_file> <output_file> <additional commands> The parameter should_transfer_files = YES instructs HTCondor to use the HTCondor file transfer method to transfer the Executable to the remote host and the job files Error (stderr) , Output (stdout) and Log back to your directory on the submit host. You will have a number of options to transfer code executables and input/output files to the remote worker node, described in the next section. You can submit the job script to the OSG via the HTCondor command on the Snowmass login node as: condor_submit myjob.submit , which will return a unique <JobID> number. You can use the <JobID> to query the status of your job with condor_q <JobID> For an introduction on managing your jobs with HTCondor we refer to this presentation by the OSG.","title":"Job Submissions to the OSG"},{"location":"job_submission/#job-submission-guidelines","text":"If your application/code was built or depends on modules used on the snowmass21 login node and it dynamically links against libraries of the module environment you would need to ensure that these modules are also availablle and loaded on the remote worker node. To do so: Insert the following parameter in your submission script: Requirements = (HAS_MODULES =?= TRUE) . This will request a worker node on a site where the OSG modules are available Before you invoke your executable inside the run.sh script load the modules as: module load module1 module2 We discuss modules with a little bit more detail in here . You must always populate the project name field, e.g. +ProjectName=\"snowmass21.energy\" , in your HTCondor submission script to: Ensure your job is validated for HTCondor to run it on the OSG grid Job statistics are properly collected and displayed on the OSG monitoring dashboard for the snowmass project.","title":"Job Submission Guidelines"},{"location":"job_submission/#the-tutorial-command","text":"OSG provides a list of tutorials available as repositories on Github. These tutorials are designed for OSG submit nodes, are tested regularly and should work as is, but if you experience any issues please contact us. Users on the Snowmass login node should be able to run most of those.","title":"The Tutorial Command"},{"location":"job_submission/#list-of-options-for-the-tutorial-command","text":"From the Snowmass21 Connect login node, the following tutorial commands are available: $ tutorial $ tutorial list $ tutorial info <tutorial-name> $ tutorial <tutorial-name>","title":"List of options for the tutorial command"},{"location":"job_submission/#available-tutorials","text":"The following tutorials pre-installed. Some additional tutorials specific to Snowmass21 will be deployed there as well. Tutorials that will not work on the Snowmass login node at present are struckthrough. To see what is currently available: $ tutorial list Currently available tutorials: R ...................... Estimate Pi using the R programming language R-addlibSNA ............ Shows how to add R external libraries for the R jobs on the OSG ScalingUp-Python ....... Scaling up compute resources - Python example to optimize a function on grid points annex .................. None blast-split ............ How to run BLAST on the OSG by splitting a large input file connect-client ......... Demonstrates how to use the connect client for remote job submission dagman-wordfreq ........ DAGMan based wordfreq example dagman-wordfreq-stash .. DAGMan based wordfreq - data from stash error101 ............... Use condor_q -better-analyze to analyze stuck jobs exitcode ............... Use HTCondor's periodic_release to retry failed jobs htcondor-transfer ...... Transfer data via HTCondor's own mechanisms matlab-HelloWorld ...... Creating standalone MATLAB application - Hello World nelle-nemo ............. Running Nelle Nemo's goostats on the grid oasis-parrot ........... Software access with OASIS and Parrot octave ................. Matrix manipulation via the Octave programming language osg-locations .......... Tutorial based on OSG location exercise from the User School pegasus ................ An introduction to the Pegasus job workflow manager photodemo .............. A complete analysis workflow using HTTP transfer quickstart ............. How to run your first OSG job root ................... Inspect ntuples using the ROOT analysis framework scaling ................ Learn to steer jobs to particular resources scaling-up-resources ... A simple multi-job demonstration software ............... Software access tutorial tensorflow-matmul ...... Tensorflow math operations as a singularity container job on the OSG - matrix multiplication Tutorials stash-cvmfs and stash-http do not work on the Snowmass Connect Login node at present. We are working to replace with working variants.","title":"Available Tutorials"},{"location":"job_submission/#install-and-setup-a-tutorial","text":"On the Snowmass Connect login node, create a directory, cd to it, and invoke the command: $ tutorial <tutorial-name> This command will clone the tutorial repository to your current working directory. cd to the repository directory and follow the steps described in the readme.md file. Alternatively, you can view the readme.md file at the tutorial's corresponding GitHub page.","title":"Install and Setup a Tutorial"},{"location":"software/","text":"Access to Software You can access software on the Snomwass21 Connect node from: Local installations in /software CVMFS in commonly used repos for the High Energy Physics community OSG module environment For applications that need to run on OSG grid, only options 2) and 3) are viable. Two new CVMFS repos contain applications deployed for OSG jobs /cvmfs/snowmass21.opensciencegrid.org/software and /cvmfs/singularity.opensciencegrid.org/snowmass21software/ . The OSG Module Environment Many commonly used software packages and libraries are provided on the OSG through the module command. OSG modules are made available through the OSG Application Software Installation Service (OASIS) CVMFS repo. Use the module avail command on the Snowmass21 Connect node to see what software and libraries are available: module avail . Use the load command to load a specific module: module load <name_of_module . For example, in the Job Submission Guidelines , users need to load the stashcache module module load stashcache in order to the use the stashcp command on a remote node and transfer data there. There are two things required in order to use modules in your HTCondor job. 1. Create an execution script for the job. The script should load the needed modules before running the main application. 2. Include the following requirements in the HTCondor submission script: Requirements = (HAS_MODULES =?= TRUE) or Requirements = [Other requirements ] && (HAS_MODULES =?= TRUE) Running Delphes Snowmass21 login node (local) For local calculations on the Snowmass Connect node, Delphes is installed in /software/Delphes-3.4.2 . You must first execute the following setup script: source /cvmfs/sft.cern.ch/lcg/views/LCG_92/x86_64-slc6-gcc62-opt/setup.sh before using the executables. Export the installation path and adjust the LD_LIBRARY_PATH of the supporting libraries as: export delphes_install=/software/Delphes-3.4.2 module use /software/modulefiles/ module load gcc-8.2.0 export LD_LIBRARY_PATH=/cvmfs/sft.cern.ch/lcg/views/LCG_92/x86_64-slc6-gcc62-opt/lib:$LD_LIBRARY_PATH export PATH=$delphes_install:$PATH Running Delphes with HepMC input files: $delphes_install/DelphesHepMC $delphes_install/cards/delphes/card_CMS.tcl delphes_output.root <input.hepmc> Running Delphes with STDHEP (XDR) input files: $delphes_install DelphesSTDHEP $delphes_install/cards/delphes/delphes_card_CMS.tcl delphes_output.root <input.hep> Running Delphes with LHEF input files: $delphes_install/DelphesLHEF $delphes_install/cards/delphes/delphes_card_CMS.tcl delphes_output.root <input.lhef> Running Delphes with files accessible via HTTP. The following example downloads a test HepMC input file and runs the executable. curl http://cp3.irmp.ucl.ac.be/~demin/test.hepmc.gz | gunzip | $delphes_install/DelphesHepMC $delphes_install/cards/delphes_card_CMS.tcl delphes_output.root Delphes on the OSG For jobs on OSG, it recommended you use the singularity container image hosted /cvmfs/singularity.opensciencegrid.org/snowmass21software/delphes-osg:latest . The image contains the full installation of the software which includes the examplles folder. The following example is a submission script which will request the availability of Singularity as a requirement on the remote worker node and loads the image for your job. Universe = Vanilla Executable = run.sh Requirements = HAS_SINGULARITY == TRUE +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/snowmass21software/delphes-osg:latest\" Error = output.err.$(Cluster)-$(Process) Output = output.out.$(Cluster)-$(Process) Log = output.log.$(Cluster) should_transfer_files = YES WhenToTransferOutput = ON_EXIT request_cpus = 1 request_memory = 1 GB +ProjectName=\"snowmass21.energy\" Queue 1 The executable script, run.sh, contains the HTTP listed above for local jobs. #!/bin/bash curl http://cp3.irmp.ucl.ac.be/~demin/test.hepmc.gz | gunzip | DelphesHepMC /opt/Delphes-3.4.2/cards/delphes_card_CMS.tcl ~/delphes_output.root There is no need to source any external environment and all Delphes executables have been added to $PATH as part of the image. Running Whizard Snowmass21 login node (local) Whizard is installed on the snowmass21 submit node in /software/ee_gg. You must set up your environment before by running the following on the submit node: module use /software/modulefiles/ module load gcc-8.2.0 export LD_LIBRARY_PATH=/software/ee_gen/./packages/OpenLoops/lib:$LD_LIBRARY_PATH export PATH=/software/ee_gen/bin:$PATH Examples are contained in this directory: /software/ee_gen/share/whizard/examples. The whizard executable will be in your $PATH. You can run an example from your home directory as: whizard /software/ee_gen/share/whizard/examples/LEP_cc10.sin Whizzard on the OSG Whizard is also available over cvmfs in /cvmfs/snowmass21.opensciencegrid.org/ee_gg. To run on the grid, ensure that your submit script has Requirements = (HAS_MODULES =?= TRUE) . You must also source the setup script in /cvmfs/snowmass21.opensciencegrid.org/ee_gg/setup.sh which will set up your environment. An example of an OSG job submission for whizard is inlined below: Submit script: Universe = Vanilla Executable = run.sh Requirements = (HAS_CVMFS =?= TRUE) && (OSGVO_OS_STRING == \"RHEL 7) && (HAS_MODULES =?= TRUE) Error = output.err.$(Cluster)-$(Process) Output = output.out.$(Cluster)-$(Process) Log = output.log.$(Cluster) should_transfer_files = YES WhenToTransferOutput = ON_EXIT request_cpus = 1 request_memory = 1 GB +ProjectName=\"snowmass21.energy\" Queue 1 Execution script (run.sh from the submit script above): #!/bin/bash source /cvmfs/snowmass21.opensciencegrid.org/ee_gg/setup.sh whizard /cvmfs/snowmass21.opensciencegrid.org//ee_gen/share/whizard/examples/LEP_cc10.sin","title":"Access to Software"},{"location":"software/#access-to-software","text":"You can access software on the Snomwass21 Connect node from: Local installations in /software CVMFS in commonly used repos for the High Energy Physics community OSG module environment For applications that need to run on OSG grid, only options 2) and 3) are viable. Two new CVMFS repos contain applications deployed for OSG jobs /cvmfs/snowmass21.opensciencegrid.org/software and /cvmfs/singularity.opensciencegrid.org/snowmass21software/ .","title":"Access to Software"},{"location":"software/#the-osg-module-environment","text":"Many commonly used software packages and libraries are provided on the OSG through the module command. OSG modules are made available through the OSG Application Software Installation Service (OASIS) CVMFS repo. Use the module avail command on the Snowmass21 Connect node to see what software and libraries are available: module avail . Use the load command to load a specific module: module load <name_of_module . For example, in the Job Submission Guidelines , users need to load the stashcache module module load stashcache in order to the use the stashcp command on a remote node and transfer data there. There are two things required in order to use modules in your HTCondor job. 1. Create an execution script for the job. The script should load the needed modules before running the main application. 2. Include the following requirements in the HTCondor submission script: Requirements = (HAS_MODULES =?= TRUE) or Requirements = [Other requirements ] && (HAS_MODULES =?= TRUE)","title":"The OSG Module Environment"},{"location":"software/#running-delphes","text":"","title":"Running Delphes"},{"location":"software/#snowmass21-login-node-local","text":"For local calculations on the Snowmass Connect node, Delphes is installed in /software/Delphes-3.4.2 . You must first execute the following setup script: source /cvmfs/sft.cern.ch/lcg/views/LCG_92/x86_64-slc6-gcc62-opt/setup.sh before using the executables. Export the installation path and adjust the LD_LIBRARY_PATH of the supporting libraries as: export delphes_install=/software/Delphes-3.4.2 module use /software/modulefiles/ module load gcc-8.2.0 export LD_LIBRARY_PATH=/cvmfs/sft.cern.ch/lcg/views/LCG_92/x86_64-slc6-gcc62-opt/lib:$LD_LIBRARY_PATH export PATH=$delphes_install:$PATH Running Delphes with HepMC input files: $delphes_install/DelphesHepMC $delphes_install/cards/delphes/card_CMS.tcl delphes_output.root <input.hepmc> Running Delphes with STDHEP (XDR) input files: $delphes_install DelphesSTDHEP $delphes_install/cards/delphes/delphes_card_CMS.tcl delphes_output.root <input.hep> Running Delphes with LHEF input files: $delphes_install/DelphesLHEF $delphes_install/cards/delphes/delphes_card_CMS.tcl delphes_output.root <input.lhef> Running Delphes with files accessible via HTTP. The following example downloads a test HepMC input file and runs the executable. curl http://cp3.irmp.ucl.ac.be/~demin/test.hepmc.gz | gunzip | $delphes_install/DelphesHepMC $delphes_install/cards/delphes_card_CMS.tcl delphes_output.root","title":"Snowmass21 login node (local)"},{"location":"software/#delphes-on-the-osg","text":"For jobs on OSG, it recommended you use the singularity container image hosted /cvmfs/singularity.opensciencegrid.org/snowmass21software/delphes-osg:latest . The image contains the full installation of the software which includes the examplles folder. The following example is a submission script which will request the availability of Singularity as a requirement on the remote worker node and loads the image for your job. Universe = Vanilla Executable = run.sh Requirements = HAS_SINGULARITY == TRUE +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/snowmass21software/delphes-osg:latest\" Error = output.err.$(Cluster)-$(Process) Output = output.out.$(Cluster)-$(Process) Log = output.log.$(Cluster) should_transfer_files = YES WhenToTransferOutput = ON_EXIT request_cpus = 1 request_memory = 1 GB +ProjectName=\"snowmass21.energy\" Queue 1 The executable script, run.sh, contains the HTTP listed above for local jobs. #!/bin/bash curl http://cp3.irmp.ucl.ac.be/~demin/test.hepmc.gz | gunzip | DelphesHepMC /opt/Delphes-3.4.2/cards/delphes_card_CMS.tcl ~/delphes_output.root There is no need to source any external environment and all Delphes executables have been added to $PATH as part of the image.","title":"Delphes on the OSG"},{"location":"software/#running-whizard","text":"","title":"Running Whizard"},{"location":"software/#snowmass21-login-node-local_1","text":"Whizard is installed on the snowmass21 submit node in /software/ee_gg. You must set up your environment before by running the following on the submit node: module use /software/modulefiles/ module load gcc-8.2.0 export LD_LIBRARY_PATH=/software/ee_gen/./packages/OpenLoops/lib:$LD_LIBRARY_PATH export PATH=/software/ee_gen/bin:$PATH Examples are contained in this directory: /software/ee_gen/share/whizard/examples. The whizard executable will be in your $PATH. You can run an example from your home directory as: whizard /software/ee_gen/share/whizard/examples/LEP_cc10.sin","title":"Snowmass21 login node (local)"},{"location":"software/#whizzard-on-the-osg","text":"Whizard is also available over cvmfs in /cvmfs/snowmass21.opensciencegrid.org/ee_gg. To run on the grid, ensure that your submit script has Requirements = (HAS_MODULES =?= TRUE) . You must also source the setup script in /cvmfs/snowmass21.opensciencegrid.org/ee_gg/setup.sh which will set up your environment. An example of an OSG job submission for whizard is inlined below: Submit script: Universe = Vanilla Executable = run.sh Requirements = (HAS_CVMFS =?= TRUE) && (OSGVO_OS_STRING == \"RHEL 7) && (HAS_MODULES =?= TRUE) Error = output.err.$(Cluster)-$(Process) Output = output.out.$(Cluster)-$(Process) Log = output.log.$(Cluster) should_transfer_files = YES WhenToTransferOutput = ON_EXIT request_cpus = 1 request_memory = 1 GB +ProjectName=\"snowmass21.energy\" Queue 1 Execution script (run.sh from the submit script above): #!/bin/bash source /cvmfs/snowmass21.opensciencegrid.org/ee_gg/setup.sh whizard /cvmfs/snowmass21.opensciencegrid.org//ee_gen/share/whizard/examples/LEP_cc10.sin","title":"Whizzard on the OSG"}]}